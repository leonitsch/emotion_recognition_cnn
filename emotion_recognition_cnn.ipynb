{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot\n",
    "from PIL import Image\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import os, os.path\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting for the Training run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters to set for the run\n",
    "image_size = 128\n",
    "train_ds_link = \"path_to_train_dataset\"\n",
    "test_ds_link = \"path_to_test_dataset\"\n",
    "validation_split = 0.1\n",
    "color_mode = \"grayscale\"\n",
    "learning_rate=0.005\n",
    "name_of_run = \"name_of_your_run\"\n",
    "epochs = 25\n",
    "batch_size = 32\n",
    "\n",
    "reduce_lr = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Start reading the Training Dataset...\")\n",
    "\n",
    "imdg = tf.keras.preprocessing.image.ImageDataGenerator(validation_split=validation_split)#, vertical_flip=True, rotation_range=20) \n",
    "\n",
    "train_ds = tf.keras.preprocessing.image.DirectoryIterator(\n",
    "    train_ds_link,\n",
    "    #labels=\"inferred\",\n",
    "    image_data_generator=imdg,\n",
    "    class_mode=\"categorical\",\n",
    "    #class_names=None,\n",
    "    color_mode=color_mode,\n",
    "    batch_size=batch_size,\n",
    "    target_size=(image_size,image_size),\n",
    "    shuffle=True,\n",
    "    seed=123,\n",
    "    interpolation=\"bilinear\",\n",
    "    follow_links=False,\n",
    "    #validation_split=0.2,\n",
    "    subset=\"training\"\n",
    ")\n",
    "print(\"Start reading the Validation Dataset...\")\n",
    "\n",
    "validation_ds = tf.keras.preprocessing.image.DirectoryIterator(\n",
    "    train_ds_link,\n",
    "    #labels=\"inferred\",\n",
    "    image_data_generator=imdg,\n",
    "    class_mode=\"categorical\",\n",
    "    #class_names=None,\n",
    "    color_mode=color_mode,\n",
    "    batch_size=batch_size,\n",
    "    target_size=(image_size,image_size),\n",
    "    shuffle=True,\n",
    "    seed=123,\n",
    "    interpolation=\"bilinear\",\n",
    "    follow_links=False,\n",
    "    #validation_split=0.2,\n",
    "    subset=\"validation\"\n",
    ")\n",
    "\n",
    "print(\"Finished reading the Datasets!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_per_class():\n",
    "    n = []\n",
    "    for i in range(6):\n",
    "        DIR = train_ds_link + str(i+1) + \"/\"\n",
    "        x = len([name for name in os.listdir(DIR) if os.path.isfile(os.path.join(DIR,name))])\n",
    "        n.append(x)\n",
    "    return n\n",
    "\n",
    "n = get_n_per_class()\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight_class1 = number_samples_in_dataset /(number_of_classes * number_samples_class1)\n",
    "def class_weights(n):\n",
    "    number_samples_in_dataset = sum(n)\n",
    "    print(\"Number of total samples: \" + str(number_samples_in_dataset))\n",
    "    class_w = []\n",
    "    number_of_classes = 6\n",
    "    labels = [0,1,2,3,4,5]\n",
    "    for i in n:\n",
    "        print(i)\n",
    "        class_w.append(number_samples_in_dataset / (number_of_classes * i))\n",
    "    return dict(zip(labels,class_w))\n",
    "\n",
    "class_w = class_weights(n)\n",
    "print(class_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Input, Dropout, Flatten, Conv2D\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, MaxPooling2D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64, (3,3), padding= 'same', input_shape= (image_size, image_size, 1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, (5,5), padding= 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(512, (3,3), padding= 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(512, (3,3), padding= 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(512))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "opt = Adam(lr=learning_rate)\n",
    "\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics= ['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try_name = name_of_run\n",
    "Path(\"./\" + try_name).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "checkpoint = ModelCheckpoint(try_name + \"model_weights.h5\", monitor='val_accuracy', save_weights_only=True, mode='max' , verbose=1 )\n",
    "callbacks = [checkpoint]\n",
    "if(reduce_lr):\n",
    "    reduce_lr_ = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience = 2, min_lr = 0.00001, model = 'auto')\n",
    "    callbacks.append(reduce_lr_)\n",
    "\n",
    "history = model.fit(train_ds, validation_data=validation_ds,epochs=epochs, verbose=1,callbacks=callbacks,class_weight=class_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Model\n",
    "model_json = model.to_json()\n",
    "with open(try_name + \"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(try_name + \"end_model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot training metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "plt.savefig(try_name + 'acc.png')\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "plt.savefig(try_name + 'loss.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdg = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "\n",
    "test_ds = tf.keras.preprocessing.image.DirectoryIterator(\n",
    "    test_ds_link,\n",
    "    #labels=\"inferred\",\n",
    "    image_data_generator=imdg,\n",
    "    class_mode=\"categorical\",\n",
    "    #class_names=None,\n",
    "    color_mode=color_mode,\n",
    "    batch_size=batch_size,\n",
    "    target_size=(image_size,image_size),\n",
    "    shuffle=False,\n",
    "    seed=123,\n",
    "    interpolation=\"bilinear\",\n",
    "    follow_links=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = model.evaluate(test_ds, verbose=1)\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate confusion matrix, precision, recall and f1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights('P003_128x128/model_weights.h5')\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "Y_pred = model.predict(test_ds, verbose=1)\n",
    "\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "print('Confusion Matrix')\n",
    "cm = confusion_matrix(test_ds.classes, y_pred)\n",
    "print(cm)\n",
    "print('Classification Report')\n",
    "target_names = [\"Happiness\", \"Sadness\", \"Surprise\", \"Fear\", \"Disgust\", \"Anger\"]\n",
    "print(classification_report(test_ds.classes, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot confusion matrix with absolute values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "sn.set(font_scale=1.4)\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(cm, xticklabels=target_names, yticklabels=target_names, annot=True, annot_kws={\"size\": 14},fmt=\"d\")\n",
    "plt.ylabel(\"True label\")\n",
    "plt.xlabel(\"Predicted label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot confusion matrix with percentage values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "sn.set(font_scale=1.4)\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(cm/np.sum(cm), xticklabels=target_names, yticklabels=target_names, annot=True, annot_kws={\"size\": 14},fmt=\".2%\")\n",
    "plt.ylabel(\"True label\")\n",
    "plt.xlabel(\"Predicted label\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
